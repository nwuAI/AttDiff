# celebahq.yml
gan_disc_type: vagan_clip  # Type of GAN discriminator
gan_loss_type: multilevel_sigmoid_s  # Type of GAN loss function
lambda_gan: 0.5  # Weight for GAN loss
lambda_style: 240  # Weight for style loss
lambda_lpips: 5  # LPIPS loss weight
lambda_lpips_pred: 5  # LPIPS prediction loss weight
lambda_lpips_fusion: 5  # LPIPS fusion loss weight
lambda_l2: 1.0  # L2 loss weight
lambda_l2_pred: 1.0  # L2 prediction loss weight
lambda_l2_fusion: 1.0  # L2 fusion loss weight
lambda_clipsim: 5.0  # CLIP similarity loss weight

# Dataset paths
train_dataset_folder: '/data/hn/DataSets/CelebAMask-HQ'                    # Path to training dataset
val_dataset_folder: '/data/hn/DataSets/CelebAMask-HQ/val256'               # Path to validation dataset
attr: '/data/hn/DataSets/CelebAMask-HQ/CelebAMask-HQ-attribute-anno.txt'   # Path to attribute annotations
mask_folder: '/data/hn/DataSets/mask/liu-mask/testing_mask_dataset'  # Path to mask dataset

# Logging and model-related parameters
log_file: '../log.txt'  # Path to the log file
model_url: null  # URL for the model (if applicable)

# Hyperparameters and training settings
attr_random_rate: 0.4  # Randomization rate for attribute selection
eval_freq: 100  # Frequency of evaluation during training
track_val_fid: true  # Whether to track FID score on validation set
RESTORE: false  # Whether to restore from a previous checkpoint
num_samples_eval: 100  # Number of samples for evaluation
viz_freq: 100  # Frequency of visualizing the outputs during training
tracker_project_name: 'i2i'  # Project name for tracking in WandB

# Pretrained model configuration
pretrained_model_name_or_path: '/data/hn/code/AttrDiff/sd-turbo_net'  # Path to pretrained model
revision: null  # Revision ID of the model (if applicable)
variant: null  # Model variant (if applicable)
tokenizer_name: null  # Tokenizer name (if applicable)

# LoRA configurations
lora_rank_unet: 8  # LoRA rank for U-Net model
lora_rank_vae: 4  # LoRA rank for VAE model

# Training batch size and directory
train_batch_size: 1  # Batch size per device
output_dir: './output'  # Directory for output and pretrained
cache_dir: null  # Directory for caching
seed: null  # Random seed for reproducibility
resolution: 256  # Input image resolution (e.g., 256x256)
num_training_epochs: 12  # Number of training epochs
max_train_steps: 10000  # Maximum number of training steps
checkpointing_steps: 200  # Frequency of checkpoint saving
gradient_accumulation_steps: 1  # Number of steps to accumulate gradients before updating
gradient_checkpointing: true  # Whether to use gradient checkpointing for memory efficiency

# Learning rate and optimizer settings
learning_rate: 0.000005  # Learning rate
lr_scheduler: constant  # Type of learning rate scheduler
lr_warmup_steps: 500  # Number of steps for learning rate warmup
lr_num_cycles: 1  # Number of learning rate cycles for cosine scheduler
lr_power: 1.0  # Power factor for polynomial learning rate scheduler

# Dataloader and optimization settings
dataloader_num_workers: 0  # Number of workers for the dataloader
adam_beta1: 0.9  # Beta1 parameter for Adam optimizer
adam_beta2: 0.999  # Beta2 parameter for Adam optimizer
adam_weight_decay: 0.01  # Weight decay for Adam optimizer
adam_epsilon: 0.00000001  # Epsilon for Adam optimizer
max_grad_norm: 1.0  # Maximum gradient norm for gradient clipping
allow_tf32: true  # Whether to allow TF32 on Ampere GPUs

# Logging and tracking
report_to: wandb  # Reporting platform (WandB in this case)
set_grads_to_none: true  # Whether to set gradients to None during backpropagation

# Precision and memory settings
mixed_precision: null  # Mixed precision mode ('fp16'/'bf16'/'no', can be null if not used)
enable_xformers_memory_efficient_attention: null  # Whether to use xformers memory-efficient attention
